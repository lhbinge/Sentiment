B[1, 2] <- 0.8
View(B)
##Generating the VAR(2) model
var2<-ARMA(A=Apoly,B=B)
##Simulating 500 observations
varsim<-simulate(var2,sampleT=500,noise=list(w=matrix(rnorm(1000),
nrow=500,ncol=2)),rng=list(seed=c(123456)))
##Obtaining the generated series
vardat<-matrix(varsim$output,nrow=500,ncol=2)
colnames(vardat)<-c("y1","y2")
##Plotting the series
plot.ts(vardat,main="",xlab="")
##Estimating the model
varsimest<-VAR(vardat,p=1,type="none",season=NULL,exogen=NULL)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = FALSE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
B<-diag(2)
B[1, 2] <- 0.8
##Generating the VAR(2) model
var2<-ARMA(A=Apoly,B=B)
##Simulating 500 observations
varsim<-simulate(var2,sampleT=500,noise=list(w=matrix(rnorm(1000),
nrow=500,ncol=2)),rng=list(seed=c(123456)))
##Obtaining the generated series
vardat<-matrix(varsim$output,nrow=500,ncol=2)
colnames(vardat)<-c("y1","y2")
##Plotting the series
plot.ts(vardat,main="",xlab="")
##Estimating the model
varsimest<-VAR(vardat,p=1,type="none",season=NULL,exogen=NULL)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = FALSE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
View(B)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
varsimest<-VAR(vardat[,c(2,1)],p=1,type="none",season=NULL,exogen=NULL)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20, impulse = "y2", response = "y1",
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
irf.y1 <- irf(varsimest, n.ahead = 20, impulse = "y2", response = "y2",
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
##Estimating the model
varsimest<-VAR(vardat,p=1,type="none",season=NULL,exogen=NULL)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20, impulse = "y2", response = "y2",
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20, impulse = "y2", response = "y1",
ortho = TRUE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
?irf
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20
ortho = FALSE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = FALSE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
##Estimating the model
varsimest<-VAR(vardat[,c(2,1)],p=1,type="none",season=NULL,exogen=NULL)
##Impulse response analysis1
irf.y1 <- irf(varsimest, n.ahead = 20,
ortho = FALSE, cumulative = FALSE, boot = FALSE, seed = 12345)
plot(irf.y1)
set.seed(189)
n = 2000
# sale dates range from 0-10
# drawn uniformly from all possible time0, time1 combinations with time0<time1
tmat <- expand.grid(seq(0,10), seq(0,10))
tmat <- tmat[tmat[,1]<tmat[,2], ]
tobs <- sample(seq(1:nrow(tmat)),n,replace=TRUE)
time0 <- tmat[tobs,1]
time1 <- tmat[tobs,2]
timesale <- time1-time0
table(timesale)
View(tmat)
# constant variance; index ranges from 0 at time 0 to 1 at time 10
y0 <- time0/10 + rnorm(n,0,.2)
y1 <- time1/10 + rnorm(n,0,.2)
fit <- repsale(price0=y0, price1=y1, time0=time0, time1=time1)
library(McSpatial)
fit <- repsale(price0=y0, price1=y1, time0=time0, time1=time1)
# variance rises with timesale
# var(u0) = .2^2; var(u1) = (.2 + timesale/10)^2
# var(u1-u0) = var(u0) + var(u1) = 2*(.2^2) + .4*timesale/10 + (timesale^2)/100
y0 <- time0/10 + rnorm(n,0,.2)
y1 <- time1/10 + rnorm(n,0,.2+timesale/10)
par(ask=TRUE)
fit <- repsale(price0=y0, price1=y1, time0=time0, time1=time1)
fit <- repsale(price0=y0, price1=y1, time0=time0, time1=time1, stage3="abs")
timesale2 <- timesale^2
fit <- repsale(price0=y0, price1=y1, time0=time0, time1=time1, stage3="square",
stage3_xlist=~timesale+timesale2)
library(tempdisagg)
install.packages("tempdisagg")
library(tempdisagg)
?td
demo(tempdisagg)
##===============================================================================================##
## -------------------------------- COMMODITY INDEX ---------------------------------------------##
##===============================================================================================##
##=====================##
## READING IN THE DATA ##
##=====================##
library(zoo)
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(stargazer)
library(micEcon)
library(quantreg)
library(McSpatial)
library(quantmod)
library(xtable)
library(scales)
library(tseries)
library(urca)
library(lmtest)
library(grid)
#setwd("C:/Users/Laurie/OneDrive/Documents/BING/METRICS/PhD Proposal Readings/Art Price Index")
setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\PhD Proposal Readings\\Commodity Cycles\\R Commodities")
comdata <- read.csv("Commodities.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\PhD Proposal Readings\\Commodity Cycles\\R Commodities")
##===============================================================================================##
## -------------------------------- COMMODITY INDEX ---------------------------------------------##
##===============================================================================================##
##=====================##
## READING IN THE DATA ##
##=====================##
library(zoo)
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(stargazer)
library(micEcon)
library(quantreg)
library(McSpatial)
library(quantmod)
library(xtable)
library(scales)
library(tseries)
library(urca)
library(lmtest)
library(grid)
#setwd("C:/Users/Laurie/OneDrive/Documents/BING/METRICS/PhD Proposal Readings/Art Price Index")
setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\PhD Proposal Readings\\Commodity Cycles\\R Commodities")
comdata <- read.csv("Commodities.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
#comdata$date <- as.Date(comdata$date, "%Y/%m/%d")
comdata$datum <- paste(comdata$datum, comdata$Year)
comdata$date <- as.Date(as.yearmon(as.character(comdata$datum),"%B %Y"))
comdata$datum <- factor(as.yearmon(as.character(comdata$datum),"%B %Y"))
coms <- aggregate(comdata$wheat, by=list(comdata$date), FUN = function(x) sum(!is.na(x)))
for(i in colnames(comdata)[7:29]) {
coms1 <- aggregate(comdata[,i], by=list(comdata$date), FUN = function(x) sum(!is.na(x)))
coms <- merge(coms, coms1, by="Group.1",all.x=TRUE)
}
colnames(coms) <- c("Date",colnames(comdata)[6:29])
complot <- melt(coms, id="Date")
g <- ggplot(complot, aes(x=Date,value,colour=variable,fill=variable))
g <- g + geom_bar(stat="identity")
g <- g + theme(legend.title=element_blank())
g <- g + ylab("Total obs")
g <- g + theme(legend.key.size = unit(0.5,"cm"))
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
coms <- aggregate(comdata$wheat, by=list(comdata$date), FUN = function(x) sum(!is.na(x)))
for(i in colnames(comdata)[7:29]) {
coms1 <- aggregate(comdata[,i], by=list(comdata$date), FUN = function(x) sum(!is.na(x)))
coms <- merge(coms, coms1, by="Group.1",all.x=TRUE)
}
colnames(coms) <- c("Date",colnames(comdata)[6:29])
complot <- aggregate(comdata$town, by=list(comdata$date, comdata$wheat), FUN = function(x) sum(!is.na(x)))
com.plot <- function(commodity="wheat") {
complot <- aggregate(comdata[,commodity], by=list(comdata$date, comdata$town), FUN = function(x) sum(!is.na(x)))
g <- ggplot(complot, aes(x=Group.1, y=x,fill=Group.2))
g <- g + geom_bar(stat="identity")
g <- g + theme(legend.title=element_blank())
g <- g + theme(legend.key.size = unit(0.4,"cm"))
g <- g + ylab(commodity)
g <- g + xlab("Date")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
}
com.plot("wheat")
com.plot("mealies")
com.plot("eggs")
com.plot("s.horses")
install.packages("rprojroot")
getOption(repos)
getOption("repos")
##===================================================================================##
## -------------------------------- SENTIMENT ---------------------------------------##
##===================================================================================##
setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\BER Confidence Surveys\\Sentiment")
suppressMessages(library(ggplot2))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(stargazer))
suppressMessages(library(xtable))
suppressMessages(library(scales))
suppressMessages(library(quantmod))
suppressMessages(library(vars))
suppressMessages(library(tseries))
suppressMessages(library(urca))
GDPdata <- read.csv("GDP Data.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
GDPdata$Date <- as.Date(GDPdata$Date, format = "%Y/%m/%d")
datums <- read.csv("dates2.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
datums$Datum <- as.Date(datums$Datum, format = "%Y/%m/%d")
##For Grpahing Business cycles
recessions.df = read.table(textConnection(
"Peak, Trough
1990-12-31, 1993-05-30
1996-11-30, 1999-08-31
2007-11-30, 2009-08-31
2013-11-30, 2016-12-31"), sep=',',
colClasses=c('Date', 'Date'), header=TRUE)
##====================================##
## READING IN THE DATA ##
##====================================##
BER.M <- rbind.fill(read.csv("Manufacturing.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE),
read.csv("Manufacturing_pre2001.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE))
BER.M <- BER.M[,1:62]
colnames(BER.M)[1:7] <- c("region","id","sector","weight","turnover","factor","surveyQ")
##===============================##
BER.B <- read.csv("Building_corrected.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
BER.B <- BER.B[!(BER.B$surveyQ %in% c("2015Q4","2016Q1","2016Q2","2016Q3")),1:21]
##============================##
BER.R <- read.csv("Retail.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
BER.W <- read.csv("Wholesale.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
BER.T <- rbind(BER.R,BER.W)
BER.T <- rbind.fill(BER.T,read.csv("Trade_pre2001.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE))
BER.T <- BER.T[,1:21]
colnames(BER.T)[1:6] <- c("region","id","sector","weight","factor","surveyQ")
##=====================================##
BER.V <- rbind.fill(read.csv("Motor.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE),
read.csv("Motor_pre2001.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE))
BER.V <- BER.V[,1:28]
colnames(BER.V)[1:6] <- c("region","id","sector","weight","factor","surveyQ")
##===============================##
BER.S <- read.csv("Services.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
colnames(BER.S)[1:6] <- c("region","id","sector","weight","factor","surveyQ")
#==================
#AGGREGATING
#==================
#Create an unweighted stacked version of all the surveys
#Match the same or similar questions from the different surveys (see survey question examples)
#Create NAs for missing questions
tempBER.M <- cbind(BER.M[,c("id","sector","weight","factor","surveyQ","Q20","Q7A","Q7P","Q1A","Q1P","Q8A","Q8P",            "Q4A","Q4P")],"Manufacturing")
colnames(tempBER.M) <-    c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P",            "Q6A","Q6P",  "Sector")
tempBER.M[,c("Q5A","Q5P")] <- NA
tempBER.B <- cbind(BER.B[,c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P","Q5A","Q5P")],            "Construction")
colnames(tempBER.B) <-    c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P","Q5A","Q5P",              "Sector")
tempBER.B[,c("Q6A","Q6P")] <- NA
tempBER.T <- cbind(BER.T[,c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q5A","Q5P","Q8",       "Q4A","Q4P")],"Trade")
colnames(tempBER.T) <-    c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P","Q5A",      "Q6A","Q6P"  ,"Sector")
tempBER.T[,c("Q5P","Q6A","Q6P")] <- NA
tempBER.V <- cbind(BER.V[,c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P",                        "Q4A","Q4P")],"Trade")
colnames(tempBER.V) <-    c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P",                        "Q6A","Q6P"  ,"Sector")
tempBER.V[,c("Q4A","Q4P","Q5A","Q5P")] <- NA
tempBER.S <- cbind(BER.S[,c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P","Q5A","Q5P")],            "Services")
colnames(tempBER.S) <-    c("id","sector","weight","factor","surveyQ", "Q1","Q2A","Q2P","Q3A","Q3P","Q4A","Q4P","Q5A","Q5P",              "Sector")
tempBER.S[,c("Q6A","Q6P")] <- NA
BER <- tempBER.M
BER <- rbind(BER,tempBER.B,tempBER.T,tempBER.V,tempBER.S)
BER <- BER[,c(15,1:12,16,17,13,14)]
rm(tempBER.M,tempBER.B,tempBER.T,tempBER.V,tempBER.S)
rm(BER.M,BER.B,BER.T,BER.V,BER.S,BER.R,BER.W)
#Clean data
BER$surveyQ <- toupper(BER$surveyQ)
BER$sector <- factor(BER$sector) #could include labels
BER$id <- factor(BER$id)
BER$surveyQ <- factor(BER$surveyQ)
# replace 1,2,3 (Up, Same, Down) responses with 1,0,-1
for(i in 7:ncol(BER)) {
BER[,i] <- replace(BER[,i], BER[,i]==2, 0)
BER[,i] <- replace(BER[,i], BER[,i]==3,-1)
}
BER$Q1 <- replace(BER$Q1, BER$Q1==0,-1) # replace 0 (Unsatisfactory) responses with -1
##====================================================================================##
## -------------------------------- UNCERTAINTY --------------------------------------##
##====================================================================================##
se <- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x))*(length(na.omit(x))-1)) #adjust for (n-1)
##---------------------##
## Dispersion
##---------------------##
calc_uncert <- function(data) {
uncertainty <- aggregate(data[,(match("surveyQ",colnames(data))+1):ncol(data)], by=list(data$surveyQ), FUN=se)
uncertainty$Uncert_cc <- rowMeans(uncertainty[,c("Q2A","Q3A","Q4A","Q5A","Q6A")],na.rm = TRUE, dims = 1)
uncertainty$Uncert_fl <- rowMeans(uncertainty[,c("Q2P","Q3P","Q4P","Q5P","Q6P")],na.rm = TRUE, dims = 1)
#Row means for simple composite indicators (the question is which questions to include)
uncertainty <- merge(datums,uncertainty,by.x="Date",by.y="Group.1", all=TRUE)
uncertainty[,14:15] <- na.approx(uncertainty[,14:15],na.rm = FALSE)
for(t in 2:nrow(uncertainty)) { uncertainty$Disp[t-1] <- uncertainty$Uncert_fl[t-1]/uncertainty$Uncert_cc[t] }
uncertainty$Disp[t] <- NA
return(uncertainty)
}
calc_wuncert <- function(data) {
##Weighted versions
weeg.2 <- function(temp) {  #calculate weighted standard deviation for each quarter for all columns
temp <- cbind(factor=temp$factor,temp$factor*temp[(match("surveyQ",colnames(temp))+1):ncol(temp)])
#calculate total that responded up (1) and down (-1) over sum(wi) = fractions up and down
frac.up <- sapply(1:ncol(temp), function(x) sum(temp[which(temp[,x]>0),x],na.rm=TRUE))/
sapply(colnames(temp), function(x) sum(temp$factor[!is.na(temp[colnames(temp) == x])]))
frac.dn <- sapply(1:ncol(temp), function(x) sum(temp[which(temp[,x]<0),x],na.rm=TRUE))/
sapply(colnames(temp), function(x) sum(temp$factor[!is.na(temp[colnames(temp) == x])]))
#weight only by those that responded to a specific question
ind <- sqrt(frac.up-frac.dn-(frac.up+frac.dn)^2)        #this is the standard devation
return(ind)
}
w.uncertainty <- as.data.frame(t(sapply(levels(data$surveyQ), function(kwartaal) weeg.2(data[data$surveyQ==kwartaal,]))))
w.uncertainty$Uncert_cc <- rowMeans(w.uncertainty[,c("Q1","Q2A","Q3A","Q4A","Q5A","Q6A")],na.rm = TRUE, dims = 1)
w.uncertainty$Uncert_fl <- rowMeans(w.uncertainty[,c("Q2P","Q3P","Q4P","Q5P","Q6P")],na.rm = TRUE, dims = 1)
w.uncertainty <- merge(datums,w.uncertainty,by.x="Date",by.y="row.names", all=TRUE)[,-3]
w.uncertainty[,14:15] <- na.approx(w.uncertainty[,14:15],na.rm = FALSE)
for(t in 2:nrow(w.uncertainty)) { w.uncertainty$Disp[t-1] <- w.uncertainty$Uncert_fl[t-1]/w.uncertainty$Uncert_cc[t] }
w.uncertainty$Disp[t] <- NA
return(w.uncertainty)
}
uncertainty.M <- calc_uncert(BER[BER$Sector=="Manufacturing",])
w.uncertainty.M <- calc_wuncert(BER[BER$Sector=="Manufacturing",])
uncertainty.B <- calc_uncert(BER[BER$Sector=="Construction",])
w.uncertainty.B <- calc_wuncert(BER[BER$Sector=="Construction",])
uncertainty.T <- calc_uncert(BER[BER$Sector=="Trade",])
w.uncertainty.T <- calc_wuncert(BER[BER$Sector=="Trade",])
uncertainty.S <- calc_uncert(BER[BER$Sector=="Services",])
w.uncertainty.S <- calc_wuncert(BER[BER$Sector=="Services",])
uncertainty <- calc_uncert(BER)
w.uncertainty <- calc_wuncert(BER)
index_plot <- w.uncertainty[,c(2,14:16)]
#index_plot[,2:5] <- scale(index_plot[,2:5])
colnames(index_plot) <- c("Date","CC","FL","DISP")
g <- ggplot(index_plot)
g <- g + geom_line(aes(x=Date, y=CC, colour="CC"), size = 1)
g <- g + geom_line(aes(x=Date, y=FL, colour="FL"), size = 1)
g <- g + geom_line(aes(x=Date, y=DISP, colour="DISP"), size = 1)
#g <- g + theme_bw()
g <- g + labs(color="Legend text")
g <- g + geom_rect(data=recessions.df, aes(xmin=Peak, xmax=Trough, ymin=-Inf, ymax=+Inf), fill='grey', alpha=0.5)
g <- g + ylab("Indicator") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"), expand=c(0,0),
limits = as.Date(c("1990-12-31", NA)))
g <- g + theme(legend.position="bottom")
g
data <- BER[BER$Sector=="Construction",]
dups <- data[duplicated(data[,c("id","surveyQ")]) | duplicated(data[,c("id","surveyQ")], fromLast = TRUE),]
data <- data[!duplicated(data[,c("id","surveyQ")]),]
exp.error <- function(temp) { #calculate errors for each respondent
#merge to create easier format
error <- merge(datums,temp,by.x="Date",by.y="surveyQ", all.x=TRUE)
for(t in 1:nrow(error)) {
error$eQ2[t]  <- error$Q2A[(t+1)] - error$Q2P[t]
error$eQ3[t]  <- error$Q3A[(t+1)] - error$Q3P[t]
error$eQ4[t]  <- error$Q4A[(t+1)] - error$Q4P[t]
error$eQ5[t]  <- error$Q5A[(t+1)] - error$Q5P[t]
error$eQ6[t]  <- error$Q6A[(t+1)] - error$Q6P[t]
}
return(error[,c(2,grep("eQ", colnames(error)))])
}
errors <- data.frame()
for(i in levels(data$id)){
errors <- rbind(errors, exp.error(data[which(data$id==i),]))
}
idio.errors <- aggregate(errors, by=list(errors$Date), FUN=se)[-2]
View(errors)
write.csv2(erros,"Building_errors.csv")
write.csv2(errors,"Building_errors.csv")
idio.errors <- aggregate(errors, by=list(errors$Datum), FUN=se)[-2]
View(idio.errors)
idio.errors$idio <- cbind(datums,rowMeans(idio.errors[,-1],na.rm = TRUE, dims = 1))
idio.errors <- aggregate(errors, by=list(errors$Datum), FUN=se)[-2]
idio.errors$idio <- rowMeans(idio.errors[,-1],na.rm = TRUE, dims = 1)
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= function(x) {mean(x, na.rm = TRUE)^2})[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= mean, na.rm = TRUE)[-2]
View(agg.errors)
write.csv2(errors,"Building_errors.csv")
agg.errors$aggregate <- rowMeans(agg.errors[,-1],na.rm = TRUE, dims = 1)
exp.errors <- cbind(idio.errors,agg.errors[,-1])
View(exp.errors)
index_plot <- cbind(exp.errors[,c(1,7,13)])
colnames(index_plot) <- c("Date","Idiosyncratic","Aggregate")
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable))
g <- g + geom_line(size = 1)
g <- g + ylab("Indicator") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g <- g + theme(legend.position="bottom")
g
index_plot[,2:3] <- scale(na.approx(index_plot[,2:3]))
index_plot[,2:3] <- scale(index_plot[,2:3])
index_plot[,2:3] <- scale(na.approx(index_plot[,2:3], na.rm=TRUE))
index_plot[,2:3] <- scale(na.approx(index_plot[,2:3], na.rm=FALSE))
View(index_plot)
index_plot <- cbind(exp.errors[,c(1,7,13)])
index_plot[,2:3] <- scale(na.approx(index_plot[,2:3], na.rm=FALSE))
colnames(index_plot) <- c("Date","Idiosyncratic","Aggregate")
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable))
g <- g + geom_line(size = 1)
g <- g + ylab("Indicator") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g <- g + theme(legend.position="bottom")
g
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= mean^2, na.rm = TRUE)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= mean(x)^2, na.rm = TRUE)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= mean()^2, na.rm = TRUE)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN=sqr(mean), na.rm = TRUE)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= function(x) mean(x)^2, na.rm = TRUE)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= function(x) mean(x, na.rm = TRUE)^2)[-2]
agg.errors <- aggregate(errors, by=list(errors$Datum), FUN= function(x) {mean(x, na.rm = TRUE)^2})[-2]
agg.errors <- aggregate(errors[,-1], by=list(errors$Datum), FUN= function(x) {mean(x, na.rm = TRUE)^2})[-2]
idio.errors <- aggregate(errors, by=list(errors$Datum), FUN=se)
idio.errors <- aggregate(errors[,-1], by=list(errors$Datum), FUN=se)
idio.errors$idio <- rowMeans(idio.errors[,-1],na.rm = TRUE, dims = 1)
agg.errors <- aggregate(errors[,-1], by=list(errors$Datum), FUN= function(x) {mean(x, na.rm = TRUE)^2})
agg.errors$aggregate <- rowMeans(agg.errors[,-1],na.rm = TRUE, dims = 1)
exp.errors <- cbind(idio.errors,agg.errors[,-1])
index_plot <- cbind(exp.errors[,c(1,7,13)])
index_plot[,2:3] <- scale(na.approx(index_plot[,2:3], na.rm=FALSE))
colnames(index_plot) <- c("Date","Idiosyncratic","Aggregate")
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable))
g <- g + geom_line(size = 1)
g <- g + ylab("Indicator") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g <- g + theme(legend.position="bottom")
g
idio.errors <- aggregate(errors[,-1], by=list(errors$Datum), FUN=se)
idio.errors[,2:6] <- scale(na.approx(idio.errors[,2:6], na.rm=FALSE))
idio.errors$idio <- rowMeans(idio.errors[,-1],na.rm = TRUE, dims = 1)
idio.errors[,2:6] <- scale(idio.errors[,2:6])
idio.errors$idio <- rowMeans(idio.errors[,-1],na.rm = TRUE, dims = 1)
idio.errors$idio <- na.approx(idio.errors$idio,na.rm=FALSE)
agg.errors <- aggregate(errors[,-1], by=list(errors$Datum), FUN= function(x) {mean(x, na.rm = TRUE)^2})
agg.errors[,2:6] <- scale(agg.errors[,2:6])
agg.errors$aggregate <- rowMeans(agg.errors[,-1],na.rm = TRUE, dims = 1)
agg.errors$aggregate <- na.approx(agg.errors$aggregate,na.rm=FALSE)
exp.errors <- cbind(idio.errors,agg.errors[,-1])
index_plot <- cbind(exp.errors[,c(1,7,13)])
colnames(index_plot) <- c("Date","Idiosyncratic","Aggregate")
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable))
g <- g + geom_line(size = 1)
g <- g + ylab("Indicator") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g <- g + theme(legend.position="bottom")
g
View(errors)
toets <- errors
toets <- na.omit(toets)
toets <- errors
toets <- na.omit(toets)
toets <- errors
toets <- complete.cases(toets)
toets <- errors
toets <- toets[complete.cases(toets),]
View(toets)
toets <- errors
toets <- toets[rowSums(is.na(toets))!=5, ]
View(toets)
dups <- data[duplicated(data[,c("id","surveyQ")]) | duplicated(data[,c("id","surveyQ")], fromLast = TRUE),]
data <- data[!duplicated(data[,c("id","surveyQ")]),]
#Compare the expectations of firms in Q7P (forward-looking) in period t to the realisations in Q7A in period t+1.
#see example survey questions
exp.error <- function(temp) { #calculate errors for each respondent
#merge to create easier format
error <- merge(datums,temp,by.x="Date",by.y="surveyQ", all.x=TRUE)
for(t in 1:nrow(error)) {
error$eQ2[t]  <- error$Q2A[(t+1)] - error$Q2P[t]
error$eQ3[t]  <- error$Q3A[(t+1)] - error$Q3P[t]
error$eQ4[t]  <- error$Q4A[(t+1)] - error$Q4P[t]
error$eQ5[t]  <- error$Q5A[(t+1)] - error$Q5P[t]
error$eQ6[t]  <- error$Q6A[(t+1)] - error$Q6P[t]
}
error <- toets[rowSums(is.na(toets))!=5, ]
return(error[,c(2,grep("eQ", colnames(error)))])
}
errors <- data.frame()
for(i in levels(data$id)){
errors <- rbind(errors, exp.error(data[which(data$id==i),]))
}
